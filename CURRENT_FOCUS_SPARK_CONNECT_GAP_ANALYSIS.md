# Spark Connect 3.5.3 Gap Analysis for Thunderduck

**Version:** 1.7
**Date:** 2025-12-12
**Purpose:** Comprehensive analysis of Spark Connect operator support in Thunderduck

---

## Executive Summary

This document provides a detailed gap analysis between Spark Connect 3.5.3's protocol specification and Thunderduck's current implementation. The analysis covers:
- **Relations** (logical plan operators)
- **Expressions** (value computation)
- **Commands** (side-effecting operations)
- **Catalog operations**

### Overall Coverage

| Category | Total Operators | Implemented | Partial | Coverage |
|----------|----------------|-------------|---------|----------|
| Relations | 40 | 20 | 1 | **50-52.5%** |
| Expressions | 16 | 9 | 0 | **56.25%** |
| Commands | 10 | 2 | 0 | **20%** |
| Catalog | 26 | 0 | 0 | **0%** |

*Partial implementations*: SubqueryAlias (needs explicit handling)

---

## 1. Relations (Logical Plan Operators)

Relations are the core building blocks of Spark Connect query plans. They represent data transformations and sources.

**Note on Actions vs Transformations**: Most Relations are **transformations** (lazy, return DataFrame). However, some Relations are **action-like** and trigger immediate execution:
- **Tail** - Must scan all data to find last N rows (M21)
- **ShowString** - Executes and returns formatted ASCII table string (M22)

### 1.1 Implemented Relations

| Relation | Proto Field | Implementation Status | Notes |
|----------|-------------|----------------------|-------|
| **Read** | `read` | âœ… Implemented | Parquet data source, named tables |
| **Project** | `project` | âœ… Implemented | Column selection and computation |
| **Filter** | `filter` | âœ… Implemented | WHERE clause predicates |
| **Aggregate** | `aggregate` | âœ… Implemented | GROUP BY + aggregations |
| **Sort** | `sort` | âœ… Implemented | ORDER BY with null ordering |
| **Limit** | `limit` | âœ… Implemented | LIMIT n |
| **Join** | `join` | âœ… Implemented | All join types (INNER, LEFT, RIGHT, FULL, SEMI, ANTI, CROSS) |
| **SetOperation** | `set_op` | âœ… Implemented | UNION, INTERSECT, EXCEPT |
| **SQL** | `sql` | âœ… Implemented | Direct SQL queries |
| **LocalRelation** | `local_relation` | âœ… Implemented | Arrow IPC data (recent addition) |
| **Deduplicate** | `deduplicate` | âœ… Implemented | DISTINCT operations |
| **ShowString** | `show_string` | âœ… Implemented | `df.show()` - formats as ASCII table (M22) |
| **Range** | `range` | âœ… Implemented | `spark.range(start, end, step)` |
| **Drop** | `drop` | âœ… Implemented | `df.drop("col")` - uses DuckDB EXCLUDE (M19) |
| **WithColumns** | `with_columns` | âœ… Implemented | `df.withColumn("name", expr)` - uses REPLACE/append (M19) |
| **WithColumnsRenamed** | `with_columns_renamed` | âœ… Implemented | `df.withColumnRenamed("old", "new")` - uses EXCLUDE+alias (M19) |
| **Offset** | `offset` | âœ… Implemented | `df.offset(n)` - uses existing Limit class (M20) |
| **ToDF** | `to_df` | âœ… Implemented | `df.toDF("a", "b", "c")` - uses positional aliasing (M20) |
| **SubqueryAlias** | `subquery_alias` | âš ï¸ Partial | Need explicit handling |
| **Tail** | `tail` | âœ… Implemented | `df.tail(n)` - ACTION, O(N) memory via TailBatchCollector (M21) |
| **Sample** | `sample` | âœ… Implemented | `df.sample(fraction, seed)` - Bernoulli sampling via DuckDB USING SAMPLE (M23) |

### 1.2 Not Implemented Relations

#### Medium Priority (Advanced Operations)

| Relation | Proto Field | Priority | Use Case |
|----------|-------------|----------|----------|
| **Hint** | `hint` | ğŸŸ¡ MEDIUM | Query hints (BROADCAST, MERGE, etc.) |
| **Repartition** | `repartition` | ğŸŸ¡ MEDIUM | `df.repartition(n)` |
| **RepartitionByExpression** | `repartition_by_expression` | ğŸŸ¡ MEDIUM | `df.repartition(col("x"))` |
| **Unpivot** | `unpivot` | ğŸŸ¡ MEDIUM | Wide-to-long transformation |
| **ToSchema** | `to_schema` | ğŸŸ¡ MEDIUM | Schema enforcement |

#### Lower Priority (NA Functions)

| Relation | Proto Field | Priority | Returns | Use Case |
|----------|-------------|----------|---------|----------|
| **NAFill** | `fill_na` | ğŸŸ¡ MEDIUM | DataFrame | `df.na.fill()` - fill nulls |
| **NADrop** | `drop_na` | ğŸŸ¡ MEDIUM | DataFrame | `df.na.drop()` - drop nulls |
| **NAReplace** | `replace` | ğŸŸ¡ MEDIUM | DataFrame | `df.na.replace()` - replace values |

#### Lower Priority (Statistics - Return DataFrames)

| Relation | Proto Field | Priority | Returns | Use Case |
|----------|-------------|----------|---------|----------|
| **StatSummary** | `summary` | ğŸŸ¢ LOW | DataFrame | `df.summary()` - stats as rows |
| **StatDescribe** | `describe` | ğŸŸ¢ LOW | DataFrame | `df.describe()` - count/mean/std/min/max |
| **StatCrosstab** | `crosstab` | ğŸŸ¢ LOW | DataFrame | `df.stat.crosstab()` - contingency table |
| **StatFreqItems** | `freq_items` | ğŸŸ¢ LOW | DataFrame | `df.stat.freqItems()` - frequent items |
| **StatSampleBy** | `sample_by` | ğŸŸ¢ LOW | DataFrame | `df.stat.sampleBy()` - stratified sample |

#### Lower Priority (Statistics - Return Scalars)

| Relation | Proto Field | Priority | Returns | Use Case |
|----------|-------------|----------|---------|----------|
| **StatCov** | `cov` | ğŸŸ¢ LOW | Double | `df.stat.cov()` - covariance |
| **StatCorr** | `corr` | ğŸŸ¢ LOW | Double | `df.stat.corr()` - correlation |
| **StatApproxQuantile** | `approx_quantile` | ğŸŸ¢ LOW | Array[Double] | `df.stat.approxQuantile()` |

#### Streaming / UDF (Future)

| Relation | Proto Field | Priority | Use Case |
|----------|-------------|----------|----------|
| **Parse** | `parse` | ğŸŸ¢ LOW | CSV/JSON parsing |
| **MapPartitions** | `map_partitions` | ğŸ”µ FUTURE | Python/Scala UDFs |
| **GroupMap** | `group_map` | ğŸ”µ FUTURE | `applyInPandas` |
| **CoGroupMap** | `co_group_map` | ğŸ”µ FUTURE | `cogroup().applyInPandas` |
| **WithWatermark** | `with_watermark` | ğŸ”µ FUTURE | Streaming watermarks |
| **ApplyInPandasWithState** | `apply_in_pandas_with_state` | ğŸ”µ FUTURE | Stateful streaming |
| **CollectMetrics** | `collect_metrics` | ğŸ”µ FUTURE | Metrics collection |
| **CommonInlineUserDefinedTableFunction** | `common_inline_user_defined_table_function` | ğŸ”µ FUTURE | Python UDTFs |

#### Cache / Catalog (Requires State Management)

| Relation | Proto Field | Priority | Use Case |
|----------|-------------|----------|----------|
| **CachedLocalRelation** | `cached_local_relation` | ğŸŸ¡ MEDIUM | Cached local data |
| **CachedRemoteRelation** | `cached_remote_relation` | ğŸŸ¡ MEDIUM | Server-side caching |
| **Catalog** | `catalog` | ğŸŸ¡ MEDIUM | Catalog operations |

---

## 2. Expressions

Expressions compute values and are used in projections, filters, aggregations, etc.

### 2.1 Implemented Expressions

| Expression | Proto Field | Implementation Status | Notes |
|------------|-------------|----------------------|-------|
| **Literal** | `literal` | âœ… Implemented | All primitive types, dates, timestamps, decimals |
| **UnresolvedAttribute** | `unresolved_attribute` | âœ… Implemented | Column references |
| **UnresolvedFunction** | `unresolved_function` | âœ… Implemented | Function calls with argument mapping |
| **Alias** | `alias` | âœ… Implemented | AS expressions |
| **Cast** | `cast` | âœ… Implemented | Type casting |
| **UnresolvedStar** | `unresolved_star` | âœ… Implemented | SELECT * |
| **ExpressionString** | `expression_string` | âœ… Implemented | Raw SQL expressions |
| **Window** | `window` | âœ… Implemented | Window functions with frame specs |
| **SortOrder** | `sort_order` | âœ… Implemented | Sort ordering (handled in RelationConverter) |

### 2.2 Not Implemented Expressions

| Expression | Proto Field | Priority | Use Case |
|------------|-------------|----------|----------|
| **UnresolvedRegex** | `unresolved_regex` | ğŸŸ¡ MEDIUM | `SELECT \`col_*\`` regex patterns |
| **LambdaFunction** | `lambda_function` | ğŸŸ¡ MEDIUM | `transform(arr, x -> x + 1)` |
| **UnresolvedNamedLambdaVariable** | `unresolved_named_lambda_variable` | ğŸŸ¡ MEDIUM | Lambda variables |
| **UnresolvedExtractValue** | `unresolved_extract_value` | ğŸŸ¡ MEDIUM | `col["key"]`, `col.field` |
| **UpdateFields** | `update_fields` | ğŸŸ¢ LOW | Struct field manipulation |
| **CallFunction** | `call_function` | ğŸŸ¢ LOW | Alternative function call syntax |
| **CommonInlineUserDefinedFunction** | `common_inline_user_defined_function` | ğŸ”µ FUTURE | Python/Scala UDFs |

### 2.3 Literal Type Support

| Literal Type | Proto Field | Status | Notes |
|--------------|-------------|--------|-------|
| Null | `null` | âœ… | |
| Binary | `binary` | âœ… | |
| Boolean | `boolean` | âœ… | |
| Byte | `byte` | âœ… | |
| Short | `short` | âœ… | |
| Integer | `integer` | âœ… | |
| Long | `long` | âœ… | |
| Float | `float` | âœ… | |
| Double | `double` | âœ… | |
| Decimal | `decimal` | âœ… | |
| String | `string` | âœ… | |
| Date | `date` | âœ… | Days since epoch |
| Timestamp | `timestamp` | âœ… | Microseconds since epoch |
| TimestampNtz | `timestamp_ntz` | âŒ | Needs implementation |
| CalendarInterval | `calendar_interval` | âŒ | Needs implementation |
| YearMonthInterval | `year_month_interval` | âŒ | Needs implementation |
| DayTimeInterval | `day_time_interval` | âŒ | Needs implementation |
| Array | `array` | âŒ | Complex type literal |
| Map | `map` | âŒ | Complex type literal |
| Struct | `struct` | âŒ | Complex type literal |

---

## 3. Commands

Commands are operations that don't return result data directly but perform side effects.

### 3.1 Implementation Status

| Command | Proto Field | Status | Priority | Use Case |
|---------|-------------|--------|----------|----------|
| **WriteOperation** | `write_operation` | âŒ Not Implemented | ğŸ”´ HIGH | `df.write.parquet()` |
| **CreateDataFrameViewCommand** | `create_dataframe_view` | âœ… Implemented | - | `df.createOrReplaceTempView()` |
| **SqlCommand** | `sql_command` | âœ… Implemented | - | `spark.sql()` (DDL + queries) |
| **WriteOperationV2** | `write_operation_v2` | âŒ Not Implemented | ğŸŸ¡ MEDIUM | Table writes |
| **RegisterFunction** | `register_function` | âŒ Not Implemented | ğŸ”µ FUTURE | UDF registration |
| **RegisterTableFunction** | `register_table_function` | âŒ Not Implemented | ğŸ”µ FUTURE | UDTF registration |
| **WriteStreamOperationStart** | `write_stream_operation_start` | âŒ Not Implemented | ğŸ”µ FUTURE | Streaming |
| **StreamingQueryCommand** | `streaming_query_command` | âŒ Not Implemented | ğŸ”µ FUTURE | Streaming |
| **StreamingQueryManagerCommand** | `streaming_query_manager_command` | âŒ Not Implemented | ğŸ”µ FUTURE | Streaming |
| **GetResourcesCommand** | `get_resources_command` | âŒ Not Implemented | ğŸŸ¢ LOW | Resource info |

---

## 4. Catalog Operations

Catalog operations allow interaction with Spark's metadata catalog.

### 4.1 Implementation Status

All catalog operations are **NOT IMPLEMENTED**:

| Operation | Proto Message | Priority | Use Case |
|-----------|---------------|----------|----------|
| **CurrentDatabase** | `current_database` | ğŸŸ¡ MEDIUM | `spark.catalog.currentDatabase` |
| **SetCurrentDatabase** | `set_current_database` | ğŸŸ¡ MEDIUM | `spark.catalog.setCurrentDatabase` |
| **ListDatabases** | `list_databases` | ğŸŸ¡ MEDIUM | `spark.catalog.listDatabases` |
| **ListTables** | `list_tables` | ğŸŸ¡ MEDIUM | `spark.catalog.listTables` |
| **ListFunctions** | `list_functions` | ğŸŸ¢ LOW | `spark.catalog.listFunctions` |
| **ListColumns** | `list_columns` | ğŸŸ¡ MEDIUM | `spark.catalog.listColumns` |
| **GetDatabase** | `get_database` | ğŸŸ¢ LOW | `spark.catalog.getDatabase` |
| **GetTable** | `get_table` | ğŸŸ¢ LOW | `spark.catalog.getTable` |
| **GetFunction** | `get_function` | ğŸŸ¢ LOW | `spark.catalog.getFunction` |
| **DatabaseExists** | `database_exists` | ğŸŸ¡ MEDIUM | `spark.catalog.databaseExists` |
| **TableExists** | `table_exists` | ğŸŸ¡ MEDIUM | `spark.catalog.tableExists` |
| **FunctionExists** | `function_exists` | ğŸŸ¢ LOW | `spark.catalog.functionExists` |
| **CreateExternalTable** | `create_external_table` | ğŸŸ¡ MEDIUM | `spark.catalog.createExternalTable` |
| **CreateTable** | `create_table` | ğŸŸ¡ MEDIUM | `spark.catalog.createTable` |
| **DropTempView** | `drop_temp_view` | ğŸ”´ HIGH | `spark.catalog.dropTempView` |
| **DropGlobalTempView** | `drop_global_temp_view` | ğŸŸ¡ MEDIUM | `spark.catalog.dropGlobalTempView` |
| **RecoverPartitions** | `recover_partitions` | ğŸŸ¢ LOW | `spark.catalog.recoverPartitions` |
| **IsCached** | `is_cached` | ğŸŸ¢ LOW | `spark.catalog.isCached` |
| **CacheTable** | `cache_table` | ğŸŸ¢ LOW | `spark.catalog.cacheTable` |
| **UncacheTable** | `uncache_table` | ğŸŸ¢ LOW | `spark.catalog.uncacheTable` |
| **ClearCache** | `clear_cache` | ğŸŸ¢ LOW | `spark.catalog.clearCache` |
| **RefreshTable** | `refresh_table` | ğŸŸ¢ LOW | `spark.catalog.refreshTable` |
| **RefreshByPath** | `refresh_by_path` | ğŸŸ¢ LOW | `spark.catalog.refreshByPath` |
| **CurrentCatalog** | `current_catalog` | ğŸŸ¢ LOW | `spark.catalog.currentCatalog` |
| **SetCurrentCatalog** | `set_current_catalog` | ğŸŸ¢ LOW | `spark.catalog.setCurrentCatalog` |
| **ListCatalogs** | `list_catalogs` | ğŸŸ¢ LOW | `spark.catalog.listCatalogs` |

---

## 5. Function Support

Thunderduck implements function name mapping between Spark and DuckDB.

### 5.1 Explicitly Mapped Functions

These functions have explicit mappings in `ExpressionConverter.mapFunctionName()`:

| Spark Function | DuckDB Function | Category |
|----------------|-----------------|----------|
| `ENDSWITH` | `ENDS_WITH` | String |
| `STARTSWITH` | `STARTS_WITH` | String |
| `CONTAINS` | `CONTAINS` | String |
| `SUBSTRING` | `SUBSTR` | String |
| `RLIKE` | `REGEXP_MATCHES` | String |
| `YEAR/MONTH/DAY` | Same | Date/Time |
| `DAYOFMONTH` | `DAY` | Date/Time |
| `DAYOFWEEK/DAYOFYEAR` | Same | Date/Time |
| `HOUR/MINUTE/SECOND` | Same | Date/Time |
| `DATE_ADD/DATE_SUB` | Same | Date/Time |
| `DATEDIFF` | `DATE_DIFF` | Date/Time |
| `RAND` | `RANDOM` | Math |
| `POW` | `POWER` | Math |
| `LOG` | `LN` | Math |
| `LOG10/LOG2` | Same | Math |
| `STDDEV` | `STDDEV_SAMP` | Aggregate |
| `STDDEV_POP/STDDEV_SAMP` | Same | Aggregate |
| `VAR_POP/VAR_SAMP` | Same | Aggregate |
| `VARIANCE` | `VAR_SAMP` | Aggregate |
| `COLLECT_LIST/COLLECT_SET` | `LIST` | Aggregate |

### 5.2 Window Functions

Supported window functions:
- `ROW_NUMBER`, `RANK`, `DENSE_RANK`, `PERCENT_RANK`, `NTILE`, `CUME_DIST`
- `LAG`, `LEAD`, `FIRST_VALUE`, `LAST_VALUE`, `NTH_VALUE`

### 5.3 Binary/Unary Operators

Fully supported:
- Arithmetic: `+`, `-`, `*`, `/`, `%`
- Comparison: `=`, `==`, `!=`, `<>`, `<`, `<=`, `>`, `>=`
- Logical: `AND`, `OR`, `NOT`, `&&`, `||`, `!`
- Null checks: `ISNULL`, `ISNOTNULL`

### 5.4 Special Expression Handling

| Feature | Status | Notes |
|---------|--------|-------|
| `ISIN` / `IN` | âœ… | Converted to SQL IN clause |
| `WHEN` / `CASE_WHEN` | âœ… | Converted to CASE statement |
| `OTHERWISE` | âœ… | ELSE clause handling |

---

## 6. Implementation Recommendations

### Phase 1: Critical Gaps (High Priority)

These are commonly used operations that users will expect to work:

1. ~~**Range** - Generate sequences~~ âœ… Implemented (2025-12-10)
2. ~~**CreateDataFrameViewCommand** - Temp view creation~~ âœ… Implemented
3. ~~**SqlCommand** - DDL support~~ âœ… Implemented (2025-12-10)
4. ~~**Drop** - Drop columns from DataFrame~~ âœ… Implemented (M19, 2025-12-10)
5. ~~**WithColumns** - Add/replace columns~~ âœ… Implemented (M19, 2025-12-10)
6. ~~**WithColumnsRenamed** - Rename columns~~ âœ… Implemented (M19, 2025-12-10)
7. ~~**Offset** - Required for pagination~~ âœ… Implemented (M20, 2025-12-10)
8. ~~**ToDF** - Rename all columns~~ âœ… Implemented (M20, 2025-12-10)
9. **Sample** - Random sampling
10. **WriteOperation** - Write to files/tables

**Estimated effort:** 1 week

### Phase 2: DataFrame NA/Stat Functions (Medium Priority)

1. **NAFill**, **NADrop**, **NAReplace** - Null handling
2. **Hint** - Query optimization hints
3. **Repartition**, **RepartitionByExpression** - Partitioning
4. **Unpivot** - Data reshaping
5. **SubqueryAlias** - Proper alias handling

**Estimated effort:** 1-2 weeks

### Phase 3: Complex Types & Expressions (Medium Priority)

1. **UnresolvedExtractValue** - Struct/Array/Map access
2. **LambdaFunction** - Array transform operations
3. **Complex literal types** (Array, Map, Struct)
4. **Interval types** (CalendarInterval, etc.)

**Estimated effort:** 2-3 weeks

### Phase 4: Catalog Operations (Medium Priority)

1. **DropTempView** - Critical for view management
2. **TableExists**, **DatabaseExists** - Existence checks
3. **ListTables**, **ListDatabases** - Metadata queries
4. **CreateTable**, **CreateExternalTable** - Table creation

**Estimated effort:** 2-3 weeks

### Phase 5: Statistical Functions (Lower Priority)

1. **StatDescribe**, **StatSummary** - Basic statistics
2. **StatCorr**, **StatCov** - Correlation/covariance
3. **StatCrosstab**, **StatFreqItems** - Frequency analysis

**Estimated effort:** 1-2 weeks

### Phase 6: Streaming & UDFs (Future)

1. Streaming operations
2. Python UDF support
3. Scala UDF support

**Estimated effort:** 4+ weeks

---

## 7. Key Observations

### What Triggered This Analysis

The recent contribution (commit `6da4199`) adding `LocalRelation` support demonstrates that external contributors are finding gaps when trying to use Thunderduck. The `LOCAL_RELATION` operator is used by Spark for:
- Returning pre-computed results (like `count()`)
- Creating DataFrames from Python lists
- Cached local data

### TPC-H/TPC-DS Coverage

The current implementation successfully handles TPC-H and TPC-DS queries because these benchmarks primarily use:
- Read (data sources)
- Project (column selection)
- Filter (WHERE clauses)
- Aggregate (GROUP BY)
- Sort (ORDER BY)
- Limit
- Join (various types)

These are all implemented. However, production workloads often include:
- `df.withColumn()` - âœ… Implemented (M19)
- `df.drop()` - âœ… Implemented (M19)
- `df.sample()` - NOT implemented
- `df.na.fill()` - NOT implemented

### Compatibility Concerns

1. **Error Messages**: When an unsupported operator is encountered, Thunderduck throws `PlanConversionException`. Users should receive clear error messages indicating which operator is not supported.

2. **Graceful Degradation**: Consider implementing stub handlers that return helpful error messages rather than generic exceptions.

3. **Version Compatibility**: This analysis is based on Spark Connect 3.5.3. Future Spark versions may add new operators.

---

## 8. Source Files

### Protocol Definitions

| File | Contents |
|------|----------|
| `connect-server/src/main/proto/spark/connect/relations.proto` | 40 relation types |
| `connect-server/src/main/proto/spark/connect/expressions.proto` | 16 expression types |
| `connect-server/src/main/proto/spark/connect/commands.proto` | 10 command types |
| `connect-server/src/main/proto/spark/connect/catalog.proto` | 26 catalog operations |

### Implementation Files

| File | Contents |
|------|----------|
| `connect-server/src/main/java/com/thunderduck/connect/converter/RelationConverter.java` | Relation handling |
| `connect-server/src/main/java/com/thunderduck/connect/converter/ExpressionConverter.java` | Expression handling |
| `connect-server/src/main/java/com/thunderduck/connect/converter/PlanConverter.java` | Plan coordination |

---

## Appendix A: Quick Reference - What Works

```python
# TRANSFORMATIONS (lazy, return DataFrame, chainable):
df = spark.read.parquet("data.parquet")      # Read
df.select("col1", "col2")                     # Project
df.filter(df.col > 10)                        # Filter
df.groupBy("col").agg(sum("val"))            # Aggregate
df.orderBy("col")                             # Sort
df.limit(100)                                 # Limit
df.join(df2, "key")                           # Join
df.union(df2)                                 # SetOperation
df.distinct()                                 # Deduplicate
spark.sql("SELECT * FROM ...")                # SQL
spark.createDataFrame([(1,2),(3,4)])          # LocalRelation
spark.range(0, 100)                           # Range
df.drop("col")                                # Drop (M19)
df.withColumn("new", expr)                    # WithColumns (M19)
df.withColumnRenamed("old", "new")            # WithColumnsRenamed (M19)
df.offset(n)                                  # Offset (M20)
df.toDF("a", "b", "c")                        # ToDF (M20)

# ACTIONS (trigger execution, return values to driver):
df.tail(n)                                    # Tail (M21) - returns List[Row], O(N) memory
df.show()                                     # ShowString (M22) - formats as ASCII table
df.collect()                                  # Collect - returns all rows

# COMMANDS (side effects, no result data):
spark.sql("CREATE TEMP VIEW ...")             # DDL via SqlCommand
df.createOrReplaceTempView("view")            # CreateDataFrameViewCommand
```

## Appendix B: Quick Reference - What Doesn't Work

```python
# TRANSFORMATIONS not yet implemented (return DataFrames):
df.sample(0.1)                                # Sample
df.na.fill(0)                                 # NAFill  - returns DataFrame
df.na.drop()                                  # NADrop  - returns DataFrame
df.hint("BROADCAST")                          # Hint
df.repartition(10)                            # Repartition
df.unpivot(...)                               # Unpivot
df.describe()                                 # StatDescribe - returns DataFrame!
df.summary()                                  # StatSummary - returns DataFrame!

# COMMANDS not yet implemented (side effects):
df.write.parquet("output")                    # WriteOperation

# CATALOG not yet implemented:
spark.catalog.listTables()                    # Catalog operations
```

## Appendix C: Actions vs Transformations

**Understanding the Distinction**:

| Type | Behavior | Returns | Chainable? |
|------|----------|---------|------------|
| **Transformation** | Lazy, builds plan | DataFrame | Yes |
| **Action** | Eager, executes | List/Value | No (terminal) |
| **Command** | Side effect | None/Result | N/A |

**Key Action-Like Relations** (trigger execution):
- `tail(n)` - Must scan all data to find last N rows, returns `List[Row]`
- `show()` / ShowString - Executes and formats output, returns String
- `collect()` - Returns all rows to driver
- `count()` - Returns single Long value

**Important**: Most "stat" operations (`describe()`, `summary()`, `crosstab()`) return **DataFrames**, not scalar values! They are transformations, not actions.

**Protocol vs Semantics**: In Spark Connect protocol, both transformations AND actions are represented as Relations. The distinction is semantic (lazy vs eager), not protocol-based.

---

**Document Version:** 1.6
**Last Updated:** 2025-12-12
**Author:** Analysis generated from Spark Connect 3.5.3 protobuf definitions
**M19 Update:** Added Drop, WithColumns, WithColumnsRenamed implementations
**M20 Update:** Added Offset, ToDF implementations
**M21 Update:** Added Tail implementation (memory-efficient O(N) via TailBatchCollector)
**M22 Update:** ShowString confirmed fully implemented (was incorrectly marked partial)
**v1.5 Update:** Clarified actions vs transformations; added semantic classification
**v1.6 Update:** Corrected ShowString to fully implemented (19 relations, 1 partial)
